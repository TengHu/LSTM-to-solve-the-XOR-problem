{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI - Request for Research 2.0 (https://blog.openai.com/requests-for-research-2/)\n",
    "\n",
    "## Warmup\n",
    "\n",
    "   Train an LSTM to solve the XOR problem: that is, given a sequence of bits, determine its parity. The LSTM should consume the sequence, one bit at a time, and then output the correct answer at the sequenceâ€™s end. Test the two approaches below:\n",
    "\n",
    " *  Generate a dataset of random 100,000 binary strings of length 50. Train the LSTM; what performance do you get?\n",
    "\n",
    " *  Generate a dataset of random 100,000 binary strings, where the length of each string is independently and randomly chosen between 1 and 50. Train the LSTM. Does it succeed? What explains the difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import inspect\n",
    "import torch.utils.data as data_utils\n",
    "import pdb\n",
    "import random\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import pandas as pd\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ModuleNotFoundError:\n",
    "    import pickle\n",
    "\n",
    "torch.manual_seed(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Bits String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary(length=1, num=1, prob=0.25):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    for i in range(0, int(round(num))):\n",
    "        buf = np.random.binomial(n=1, p=prob, size=(length))    \n",
    "        inputs.append([[(i+1)%2, (i)%2] for i in buf])\n",
    "        targets.append(np.cumsum(buf) % 2)\n",
    "    return torch.Tensor(inputs), torch.Tensor(targets).unsqueeze(2)\n",
    "\n",
    "def collate_generate(sequencelength=50, batchsize=2, is_variable_length=False):\n",
    "    def pad_collate(batch):\n",
    "        data = [item[0][0:sequencelength] for item in batch]\n",
    "        target = [item[1][0:sequencelength] for item in batch]\n",
    "        length = sequencelength\n",
    "        \n",
    "        lengths_vector = [random.randint(1,length) if is_variable_length else length for i in range(0, batchsize)]\n",
    "        lengths_vector.sort(reverse=True)\n",
    "        \n",
    "        data = rnn_utils.pack_padded_sequence(torch.stack(data), lengths_vector, batch_first=True)\n",
    "        target = rnn_utils.pack_padded_sequence(torch.stack(target), lengths_vector, batch_first=True)\n",
    "        return [data, target]\n",
    "    return pad_collate\n",
    "\n",
    "def flatten(S):\n",
    "    if S == []:\n",
    "        # terminal condition\n",
    "        return S\n",
    "    elif isinstance(S[0], list):\n",
    "        # first element is list\n",
    "        return flatten(S[0]) + flatten(S[1:])\n",
    "    else:\n",
    "        # first element is not a list\n",
    "        return S[:1] + flatten(S[1:])\n",
    "    \n",
    "    \n",
    "\n",
    "def pad_and_average(lists):\n",
    "    # padding\n",
    "    N = max([len(x) for x in lists])\n",
    "    new_lists = [x + [0] * (N - len(x)) for x in lists]\n",
    "    return np.mean(new_lists ,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, feature_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=feature_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True)\n",
    "        self.h2o = nn.Linear(hidden_size, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def unpacked(self, hidden):\n",
    "        if isinstance(hidden, torch.nn.utils.rnn.PackedSequence):\n",
    "            hidden, lengths_vector = rnn_utils.pad_packed_sequence(hidden, batch_first=True)\n",
    "        return hidden\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h, h_and_c = self.rnn(inputs)\n",
    "        h = self.unpacked(h)\n",
    "        outputs = self.h2o(h)\n",
    "        return outputs, self.sig(outputs), h_and_c\n",
    "\n",
    "    def initRandomHidden(self, batch_size=1):\n",
    "        h = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        c = torch.zeros(1, batch_size, self.hidden_size)\n",
    "        return [h, c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with fixed length 50\n",
    " *  Generate a dataset of random 100,000 binary strings of length 2, 10, 30, 50. Test on fixed 50 length dataset.\n",
    "\n",
    "* batch size 20\n",
    "* Adam optimizer with learning rate 0.01\n",
    "* Cross Entropy Loss\n",
    "* The model doesn't need to learn long dependency but to learn the gates that can read and write parity information to/from state. Each model achieves 100 % within 3 epochs\n",
    "* The longer the sequence, the longer time it took to converge to optima. Main explanation is the longer the sequence, the worse gradient morphing (vanishes/explode), makes the network hard to learn toe correct signal. Also the gradient vanishing caused the gradient to be dominated by short term input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Below is some experiments result. Note that the legend (N, false) means training with sequences with fixed length N. (N, false) means training with  sequences with variable lengths [1, N]\n",
    "\n",
    "#### Cross Entropy Loss on length 50 testset\n",
    "\n",
    "![title](./xor1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss on Training set (length N)\n",
    "\n",
    "\n",
    "![title](./xor2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss on testset (length 50)\n",
    "\n",
    "![title](./xor3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss on Training set (length N)\n",
    "\n",
    "![title](./xor4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with variable length in [1,50]\n",
    "\n",
    "\n",
    "* batch size 20\n",
    "* Adam optimizer with learning rate 0.01\n",
    "* Cross Entropy Loss\n",
    "* Test model trained with fix length and variable length\n",
    " \n",
    "### Compared to training with fixed size strings, network is easier to learn with variable length strings. My explaination is the random strings form a curriculum which consists of easy(short) and hard(long) tasks. Network gets to learn to solve the 2-length xor to 50-length xor problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss of training\n",
    "\n",
    "![title](./xor6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Entropy Loss on Testset (training with fixed and variable length, testing on length 50)\n",
    "\n",
    "![title](./xor5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Training code '''\n",
    "\n",
    "print_every = 300\n",
    "save_loss_every = 4\n",
    "repeat_configuration = 5\n",
    "\n",
    "epochs = 2\n",
    "num_observation = 10000\n",
    "num_test_observation = 100\n",
    "\n",
    "hidden_size = 2\n",
    "batch_size = 20\n",
    "max_length = 100\n",
    "test_max_length = 200\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='elementwise_mean')\n",
    "\n",
    "def evaluate(model, inputs, targets, length, is_variable_length=True):\n",
    "    accuracy = 0\n",
    "    num_samples = inputs.shape[0]\n",
    "    batch_size = num_samples\n",
    "    \n",
    "    dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
    "    \n",
    "    loss = 0\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_generate(sequencelength=length, batchsize=batch_size, is_variable_length=is_variable_length),\n",
    "        pin_memory=True)\n",
    "\n",
    "    for batch_idx, data in enumerate(dataloader, 1):\n",
    "\n",
    "        '''\n",
    "        input[1] is tensor([20, 13,  9,  7,  4]) which represents\n",
    "        say batch size is 4, \n",
    "        input[1] should be [4, 2, 2, 1]: first element from all 4 sequences, second from 2 sequences... \n",
    "        last element only from first sequence.\n",
    "\n",
    "        1 1 1 1 \n",
    "        1 1 1 \n",
    "        1 \n",
    "        1 \n",
    "        '''\n",
    "        input, target = data  \n",
    "        \n",
    "        output_tensor, predict_tensor, _ = model(input)\n",
    "        \n",
    "        # unpack\n",
    "        target_tensor, lengths_vector = rnn_utils.pad_packed_sequence(target, batch_first=True)\n",
    "        \n",
    "        # pack\n",
    "        output = rnn_utils.pack_padded_sequence(output_tensor, lengths_vector, batch_first=True)\n",
    "        \n",
    "        loss += criterion(output.data, target.data)\n",
    "        \n",
    "        # we only care the final output of each string which is at index i-1 for i in lengths_vector\n",
    "        for idx_in_batch, idx_in_sequence in enumerate(lengths_vector.tolist(), 0):\n",
    "            accuracy += ((predict_tensor[idx_in_batch, idx_in_sequence-1] > 0.5).numpy() ==  target_tensor[idx_in_batch, idx_in_sequence-1].numpy())\n",
    "    return accuracy[0] / num_samples\n",
    "\n",
    "\n",
    "def evaluate_with_generated_data(model, test_length=50, num_samples=100, is_variable_length=False, prob=0.2):\n",
    "    inputs, targets = generate_binary(length=test_length, num=num_samples, prob=prob)\n",
    "    return evaluate(model, inputs, targets, test_length, is_variable_length=is_variable_length)\n",
    "\n",
    "\n",
    "'''\n",
    "Training\n",
    "'''\n",
    "lr = 0.01\n",
    "model_list = []\n",
    "\n",
    "#curriculum = [(10, False), (10, True), (20, False), (20, True), (30, False), (30, True),(50, False), (50, True)]\n",
    "#curriculum = [(5, False), (10, False), (40, False), (50, False), (50, True)]\n",
    "\n",
    "''' (Length, IsRandom, ProbabilityOf1) '''\n",
    "curriculum = [(25, False, 0.25), (25, True, 0.25), (50, False, 0.25), (50, True, 0.25)]\n",
    "\n",
    "\n",
    "test_losses_dict = {}\n",
    "losses_dict = {}\n",
    "\n",
    "\n",
    "'''testing data'''\n",
    "test_inputs, test_targets = generate_binary(length=test_max_length, num=num_test_observation, prob=0.25)\n",
    "\n",
    "\n",
    "''' testing 50 loss data '''\n",
    "test_loss_inputs, test_loss_targets = generate_binary(length=test_max_length, num=100, prob=0.25)\n",
    "\n",
    "\n",
    "def test_loss(model, inputs, targets):\n",
    "    outputs, _, _ = model(inputs)\n",
    "    return criterion(outputs, targets).item()\n",
    "\n",
    "\n",
    "for idx, curriculum_set in enumerate(curriculum): \n",
    "    length, is_random, probability = curriculum_set\n",
    "    \n",
    "    ''' training data '''\n",
    "    inputs, targets = generate_binary(length=max_length, num=num_observation, prob=probability)\n",
    "    dataset = torch.utils.data.TensorDataset(inputs, targets)\n",
    "    \n",
    "    ''' training dataloader '''\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "                    dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    num_workers=1,\n",
    "                    drop_last=True,\n",
    "                    collate_fn=collate_generate(sequencelength=length, batchsize=batch_size,  is_variable_length=is_random),\n",
    "                    pin_memory=True)\n",
    "\n",
    "    \n",
    "    \n",
    "    for repeat_configuration_idx in range(1, repeat_configuration+1): \n",
    "        \n",
    "        print (\"{} run, curriculum {}, learning rate {}, batch size {}, training length {}, probability {}\"\n",
    "               .format(repeat_configuration_idx, curriculum_set, lr, batch_size, length, probability))\n",
    "  \n",
    "        \n",
    "        ## initialize model\n",
    "        model = LSTMModel(2, hidden_size)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        # tracking stats\n",
    "        test_losses = []\n",
    "        losses = []\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            for i in range(1, epochs+1):    \n",
    "                running_loss = 0\n",
    "                for batch_idx, data in enumerate(dataloader, 1):\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    input, target = data \n",
    "                    output, _, _ = model(input)\n",
    "\n",
    "                    # unpack targets PackedSequence to get variable length vector\n",
    "                    _, lengths_vector = rnn_utils.pad_packed_sequence(target, batch_first=True)\n",
    "                    \n",
    "                    # use pack_padded_sequence function to flatten output according to variable length vector\n",
    "                    output = rnn_utils.pack_padded_sequence(output, lengths_vector, batch_first=True)\n",
    "\n",
    "                    #loss = criterion(output.data[-1].unsqueeze(0), target.data[-1].unsqueeze(0))\n",
    "                    loss = criterion(output.data, target.data)\n",
    "\n",
    "                    running_loss += loss\n",
    "\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "\n",
    "\n",
    "                    if batch_idx % save_loss_every == 0:\n",
    "                        test_losses.append(test_loss(model, test_loss_inputs, test_loss_targets))\n",
    "                        losses.append(loss.item())\n",
    "\n",
    "                    if batch_idx % print_every == 0:\n",
    "                        print (\"epoch {}, batch {}, loss {}\".format(i, batch_idx, running_loss / print_every))\n",
    "                        if running_loss / print_every < 1e-4:\n",
    "                            raise StopIteration\n",
    "                        running_loss = 0\n",
    "\n",
    "                        \n",
    "                print (\"Accuracy: {} %\".format(evaluate_with_generated_data(model, test_length=length, num_samples=100, is_variable_length=False)\n",
    "    * 100))\n",
    "            \n",
    "        except StopIteration:\n",
    "            print (\"{} curriculum early exit.\".format(curriculum_set))\n",
    "\n",
    "        print ('Test Set Accuracy: {} % \\n'.format(evaluate(model, test_inputs, test_targets, length=test_max_length, is_variable_length=False) * 100), '#' * 100)\n",
    "\n",
    "        ''' collect each repeat model run for curriculum summary'''\n",
    "        if curriculum_set not in losses_dict:\n",
    "            losses_dict[curriculum_set] = []\n",
    "        losses_dict[curriculum_set].append(losses)\n",
    "\n",
    "        \n",
    "        if curriculum_set not in test_losses_dict:\n",
    "            test_losses_dict[curriculum_set] = []\n",
    "        test_losses_dict[curriculum_set].append(test_losses)\n",
    "\n",
    "    \n",
    "    ''' curriculum loss summary '''\n",
    "    losses_dict[curriculum_set] = pad_and_average(losses_dict[curriculum_set])\n",
    "    test_losses_dict[curriculum_set] = pad_and_average(test_losses_dict[curriculum_set])\n",
    "\n",
    "    \n",
    "    ''' Save Losses: save to different  '''\n",
    "    with open(\"{}_model_losses\".format(str(curriculum_set)), 'wb') as file:\n",
    "        buffer = {}\n",
    "        buffer['model'] = model\n",
    "        buffer['losses'] = losses_dict[curriculum_set]\n",
    "        pickle.dump(buffer, file, pickle.HIGHEST_PROTOCOL)\n",
    "    print (\"{}_model_losses saved\".format(str(curriculum_set)))\n",
    "    model_list.append(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the same network on 100000 bits string with length with 2, 5, 12, 25, 50, 100 respectively and test it on 10000 bits string with variable lengths between 1 and 50. \n",
    "Two observations.\n",
    "1. Shorter the string, easier the training. Probably because to get the correct parity, network needs to memorize each bit in the string. Longer dependency is harder to learn due to gradient explosion through bptt.\n",
    "2. Loss stays still then sudden drops to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.666666666666666  %\n"
     ]
    }
   ],
   "source": [
    "# test long dependency\n",
    "# 10000...000001 \n",
    "\n",
    "accuracy = 0\n",
    "test_samples = 300\n",
    "\n",
    "for j in range(0, test_samples):\n",
    "    tt = [[0., 1.]]\n",
    "    for i in range(0, j):\n",
    "        tt.append([1., 0.])\n",
    "    tt.append([0., 1.])\n",
    "    ans = np.sum([e[1] for e in tt]) % 2\n",
    "    \n",
    "    out, predict, hiddens = model(torch.Tensor(tt).unsqueeze(0))\n",
    "    \n",
    "    #print (predict[0][-1])\n",
    "    accuracy += ((predict[0][-1] > 0.5) == ans)[0]\n",
    "    #print ((predict[0][-1] > 0.5) == ans)\n",
    "\n",
    "print (accuracy.numpy() / test_samples * 100, \" %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testset loss\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "legends = []\n",
    "for k,v in test_losses_dict.items():\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "plt.legend(legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loss\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "legends = []\n",
    "for k,v in losses_dict.items():\n",
    "    plt.plot(v)\n",
    "    legends.append(k)\n",
    "plt.legend(legends)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Vanishing gradients in a 50 length sequence '''\n",
    "\n",
    "max_length = 50\n",
    "gradient_vector = []\n",
    "\n",
    "model = LSTMModel(2, 2)\n",
    "\n",
    "# for short to long\n",
    "inputs, targets = generate_binary(length=max_length, num=10000)\n",
    "\n",
    "for length in range(1, max_length):\n",
    "    model.zero_grad()    \n",
    "    \n",
    "    input = inputs[:, (max_length - length):,  :]\n",
    "    target = targets[:, (max_length - length):,  :]\n",
    "    outputs, _ = model(input)\n",
    "    \n",
    "    loss = criterion(outputs, target).backward()\n",
    "    gradient_vector.append(model.rnn.weight_ih_l0.grad.abs().sum())\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "plt.plot([abs((curr - prev) / prev) for prev, curr in zip(gradient_vector, gradient_vector[1:])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "''' Visualize heatmap to compare testset and trainset performance '''\n",
    "import numpy as np; np.random.seed(0)\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "max_length = 500\n",
    "inputs, targets = generate_binary(length=max_length, num=1000, prob=0.25)\n",
    "\n",
    "data = pd.DataFrame(columns = curriculum)\n",
    "for i in range(2, max_length, 20):\n",
    "    buff = []\n",
    "    for idx, model in enumerate(model_list):\n",
    "        print \n",
    "        buff.append(evaluate(model, inputs, targets, length=i, is_variable_length=False))\n",
    "    data.loc[i] = buff\n",
    "\n",
    "data = data.iloc[::-1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax = sns.heatmap(data, annot=True, linewidths=.2, ax=ax) # square=True, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
